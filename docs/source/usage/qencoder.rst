Quantized Text Encoders
=======================

Nunchaku provides a quantized T5 encoder for FLUX.1 to reduce GPU memory usage.

.. literalinclude:: ../../../examples/flux.1-dev-qencoder.py
   :language: python
   :caption: Running FLUX.1-dev with Quantized T5 (`examples/flux.1-dev-qencoder.py <https://github.com/mit-han-lab/nunchaku/blob/main/examples/flux.1-dev-qencoder.py>`__)
   :linenos:
   :emphasize-lines: 11, 14

The key changes from `basic usage <../basic_usage/basic_usage>`_ are:

**Loading Quantized T5 Encoder** (line 11):
Use ``NunchakuT5EncoderModel.from_pretrained()`` to load the quantized encoder. This reduces GPU memory usage while maintaining quality. Supports local or Hugging Face remote paths.

**Pipeline Integration** (line 14):
Pass the quantized encoder to the pipeline via the ``text_encoder_2`` parameter, replacing the default T5 encoder.

.. note::

   The quantized T5 encoder currently only supports CUDA backend. Turing GPUs will be supported later.
