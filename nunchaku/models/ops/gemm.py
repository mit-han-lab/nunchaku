import torch
from ._C import ops


def svdq_gemm_w4a4(
    act: torch.Tensor,
    wgt: torch.Tensor,
    out: torch.Tensor | None,
    qout: torch.Tensor | None,
    ascales: torch.Tensor,
    wscales: torch.Tensor,
    oscales: torch.Tensor | None,
    poolout: torch.Tensor | None,
    lora_act_in: torch.Tensor,
    lora_up: torch.Tensor,
    lora_down: torch.Tensor | None,
    lora_act_out: torch.Tensor | None,
    norm_q: torch.Tensor | None,
    norm_k: torch.Tensor | None,
    rotary_emb: torch.Tensor | None,
    bias: torch.Tensor | None,
    smooth_factor: torch.Tensor | None,
    out_vk: torch.Tensor | None,
    out_linearattn: torch.Tensor | None,
    act_unsigned: bool = False,
    lora_scales: list[float] = [],
    fuse_silu: bool = False,
    fp4: bool = False,
    alpha: float = 1.0,
    wcscales: torch.Tensor | None = None,
    out_q: torch.Tensor | None = None,
    out_k: torch.Tensor | None = None,
    out_v: torch.Tensor | None = None,
    attn_tokens: int = 0,
):
    ops.gemm_w4a4(
        act,
        wgt,
        out,
        qout,
        ascales,
        wscales,
        oscales,
        poolout,
        lora_act_in,
        lora_up,
        lora_down,
        lora_act_out,
        norm_q,
        norm_k,
        rotary_emb,
        bias,
        smooth_factor,
        out_vk,
        out_linearattn,
        act_unsigned,
        lora_scales,
        fuse_silu,
        fp4,
        alpha,
        wcscales,
        out_q,
        out_k,
        out_v,
        attn_tokens,
    )
